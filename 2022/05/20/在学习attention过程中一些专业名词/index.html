<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>在学习attention过程中一些专业名词 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="首先保存一下各类常用英文词汇及解释ALSC:情感分类**存档以后好看：ACL EMNLP ** #CNN图像、音频、文本处理（将数据转化为图像进行处理）输入：二维像素阵列（图片）通过CNN黑箱输出：图片的意义&amp;nbsp在输入过程如灰度图像，计算机接收到的是-1和1.（当两个图片有一定偏差时，计算机就能够通过识别图片的特征，而以下图为例三张图为x的不同形态，找到三张图的特征形成卷积核##c">
<meta property="og:type" content="article">
<meta property="og:title" content="在学习attention过程中一些专业名词">
<meta property="og:url" content="http://example.com/2022/05/20/%E5%9C%A8%E5%AD%A6%E4%B9%A0attention%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="首先保存一下各类常用英文词汇及解释ALSC:情感分类**存档以后好看：ACL EMNLP ** #CNN图像、音频、文本处理（将数据转化为图像进行处理）输入：二维像素阵列（图片）通过CNN黑箱输出：图片的意义&amp;nbsp在输入过程如灰度图像，计算机接收到的是-1和1.（当两个图片有一定偏差时，计算机就能够通过识别图片的特征，而以下图为例三张图为x的不同形态，找到三张图的特征形成卷积核##c">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/picture/hexo-Img/%E5%8D%B7%E7%A7%AF%E5%B1%82.png">
<meta property="og:image" content="https://img-blog.csdn.net/20180902220822202?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="article:published_time" content="2022-05-20T14:37:43.000Z">
<meta property="article:modified_time" content="2022-05-26T15:56:06.956Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="卷积层，过滤器">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/picture/hexo-Img/%E5%8D%B7%E7%A7%AF%E5%B1%82.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-在学习attention过程中一些专业名词" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/20/%E5%9C%A8%E5%AD%A6%E4%B9%A0attention%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/" class="article-date">
  <time class="dt-published" datetime="2022-05-20T14:37:43.000Z" itemprop="datePublished">2022-05-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      在学习attention过程中一些专业名词
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p> <a target="_blank" rel="noopener" href="https://blog.csdn.net/Augusdi/article/details/9020235?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E4%B8%AD1%E5%92%8C-1%E8%8B%B1%E6%96%87&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-9020235.142^v10^pc_search_result_control_group,157^v4^control&spm=1018.2226.3001.4187" ><font color="red"><strong>首先保存一下各类常用英文词汇及解释</strong></font></a><br>ALSC:情感分类<br>**存档以后好看：ACL EMNLP **</p>
<p>#CNN<br>图像、音频、文本处理（将数据转化为图像进行处理）<br>输入：二维像素阵列（图片）<br>通过<strong>CNN</strong>黑箱<br>输出：图片的意义<br>&amp;nbsp在输入过程如灰度图像，计算机接收到的是-1和1.（当两个图片有一定偏差时，计算机就能够通过识别图片的特征，而以下图为例<br><img src="/picture/hexo-Img/%E5%8D%B7%E7%A7%AF%E5%B1%82.png" alt=" "><br>三张图为x的不同形态，找到三张图的特征形成卷积核<br>##channel<br>&amp;nbsp;&amp;nbsp;对于输入层而言，channel代表图像的通道数量，当输入图像为RGB彩色图像，则channel &#x3D; 3；如果是灰度图像，则channel &#x3D; 1。<br>&amp;nbsp;&amp;nbsp;对于卷积核而言，卷积核的深度 &#x3D; 卷积核channel数<br>若当前卷积层的上一层为输入层，则channel数 &#x3D; 输入图像channel数无论输入图像的深度是多少，每经过一个卷积核都将变换为一个深度为1的特征图，一个卷积层之内可定义多个卷积核，当前卷积层上的各个卷积核会对上一层输入的每feature map（特征图）分别执行卷积操作，即每个卷积核都会对应生成一个新的特征图feature map(不同的卷积核所提取的特征不同)。故而在下一层需要多少个特征图，本层就需要定义多少个卷积核，即卷积核的深度与传出的特征图的张数一致。<br>##神经元、卷积和池化、softmax<br><strong>神经元</strong>：由一个线性函数和一个<u>非线性激活函数</u>（改变数据线性关系并将数据映射到某个范围内，防止过大溢出）构成.<br><strong>全连接</strong>：所有神经元都与下层神经元有关系。（缺点：计算量大）<br>*<em>填充（padding)*<em>：在卷积过程中，输入数据边缘不能利用到如（通过3</em>3的卷积核去进行卷积5*5的数据，输出结果为3</em>3），就用0去填充输入矩阵外轮廓（p&#x3D;k-1&#x2F;2). 填充完过后输出的矩阵就会变成和输入大小相同的矩阵（特征矩阵）。<br><strong>步长（Stride）</strong>：卷积过程中跳跃的距离，一般只移动一格但是不限。<br><strong>卷积</strong>：卷积核的生成方式一般是随机的或全为0又或者通过预训练的方式提取卷积核然后通过算法去纠正卷积核到底是什么。（假设输入图像为n<em>n,卷积核(过滤器）为f</em>f，填充p，步长s,则输出<font color="red"> <strong>O&#x3D;（n-f+2p)&#x2F;s+1</strong></font>)<br>在rgb图像中channel为3，可以通过多通道的过滤器（3<em>3</em>3）卷积完成后经过cnn输出与卷积过滤器（3<em>3</em>3）数量相同的过滤器（3<em>3</em>通道数）个数。<br><strong>池化</strong>：作用：降维。&amp;emsp;  1.最大池化：取当前卷积核中数字最大的一的数，形成新的卷积核。<br><strong>Dropout层</strong>:111<br><strong>softmax</strong>:<br><img src="https://img-blog.csdn.net/20180902220822202?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"><br>假设有一个数组V， V i V_i Vi​表示V中的第i个元素，那么这个元素的softmax值为:<br><code>S i = e i ∑ j e j S_i = \frac&#123;e^i&#125;&#123;\sum_j e^j&#125; Si​=∑j​ejei</code>​  </p>
<p>该元素的softmax值，就是该元素的指数与所有元素指数和的比值。<br>##卷积层（kilter)和过滤器(filter)<br><a target="_blank" rel="noopener" href="https://medlen.blog.csdn.net/article/details/109906338">对于卷积层（kernel）和过滤器(filter)的区别</a>  </p>
<pre><code>卷积核就是由长和宽来指定的，是一个二维的概念。
而过滤器是是由长、宽和深度指定的，是一个三维的概念。
过滤器可以看做是卷积核的集合。
过滤器比卷积核高一个维度——深度。  
</code></pre>
<p>##RELU(激活函数)<br>函数公式:<font color=red size="20px"><code>f(x)=max(0,x)</code></font>;<br><code>def rectified(x)</code><br><code>return max(0.0,x);</code><br>优点：1)克服梯度消失的问题&amp;emsp;2)加快训练速度<br>缺点：1）输入负数，则完全不激活，ReLU函数死掉。&amp;emsp;12）ReLU函数输出要么是0，要么是正数，也就是ReLU函数不是以0为中心的函数<br>1.ReLU不是线性函数，为负时输出为0，为正时输出本身，而相对于tanh这些来说它的好处是能够返回一个真正意义的0而不是说近似的(所以有时也在于他相似于线性但是非线性，当神经网络的行为是线性或接近线性时，它更容易优化 )<br>2.对 MLPs，CNNs 使用 ReLU，但不是 RNNs，还有使用Tanh的LTSM都不适合使用<br>3.tanh 和 sigmoid 激活函数需要使用指数计算，而relu只用max()<br>#Residual(残差网络)<br> <font color="red"><strong><code>y=F(x,w)+x</code></strong></font><br> ResNet通过shortcut connections结构使得后面很多层能够直接学习残差，减少了很多信息的损耗和丢失：<br>通过将很多信息绕道传给输出，保护信息完整性，使得整个网络只需要学习输入和输出差别的那一部分。<br>##parameter-free<br>一种无监督聚类算法<br><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Sarfraz_Efficient_Parameter-Free_Clustering_Using_First_Neighbor_Relations_CVPR_2019_paper.pdf">Efficient Parameter-free Clustering Using First Neighbor Relations​arxiv.org </a>   </p>
<p>–  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/20/%E5%9C%A8%E5%AD%A6%E4%B9%A0attention%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/" data-id="cl3fh6ixv0000zoutfyl32bmf" data-title="在学习attention过程中一些专业名词" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">卷积层，过滤器</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/05/22/BERT%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          BERT模型学习
        
      </div>
    </a>
  
  
    <a href="/2022/05/20/attention-is-all-u-need/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Attention is all u need</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Batch-Normalization/" rel="tag">Batch-Normalization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Layer-Normalization/" rel="tag">Layer-Normalization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">卷积层，过滤器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Batch-Normalization/" style="font-size: 10px;">Batch-Normalization</a> <a href="/tags/Layer-Normalization/" style="font-size: 10px;">Layer-Normalization</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%87%E6%BB%A4%E5%99%A8/" style="font-size: 10px;">卷积层，过滤器</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/25/ACL/">ACL</a>
          </li>
        
          <li>
            <a href="/2022/05/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">吴恩达深度学习笔记</a>
          </li>
        
          <li>
            <a href="/2022/05/22/BERT%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">BERT模型学习</a>
          </li>
        
          <li>
            <a href="/2022/05/20/%E5%9C%A8%E5%AD%A6%E4%B9%A0attention%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%B8%80%E4%BA%9B%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/">在学习attention过程中一些专业名词</a>
          </li>
        
          <li>
            <a href="/2022/05/20/attention-is-all-u-need/">Attention is all u need</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>